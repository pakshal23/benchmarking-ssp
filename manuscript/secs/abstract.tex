We introduce a statistical framework to evaluate the optimality
of reconstruction algorithms for linear inverse problems based
on sparse stochastic processes. These are realistic models
of sparse signals for which sparsity-based variational
techniques perform very well. In particular, we derive Gibbs
sampling schemes to obtain optimal estimators (the posterior
mean) for L\'{e}vy processes with Laplace, Student's t- and
Bernoulli-Laplace innovations. We showcase the use of our
framework by characterizing the optimality of the popular
DnCNN model for artifact removal in deconvolution and Fourier
sampling problems. Our experimental results suggest that while
this architechture achieves near-optimal results in many settings,
heavy-tailed innovations in the signal to recover disrupt its
performance.
